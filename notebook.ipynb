{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def fillURLs():\n",
    "    with open('urlsList.json') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file(url):\n",
    "    #print(\"is a file\", url)\n",
    "    response = requests.get(str(url))\n",
    "    text = response.text\n",
    "    #print(text)\n",
    "    return text\n",
    "    \n",
    "def site(url):\n",
    "    #print(\"is a site\", url)\n",
    "    response = requests.get(str(url))\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "options = {\n",
    "    \"F\": file,\n",
    "    \"S\": site\n",
    "}\n",
    "\n",
    "def retrieve():\n",
    "    elements = fillURLs()\n",
    "    texts = []\n",
    "    for element in elements:\n",
    "        if( (element[\"type\"] == \"F\" or element[\"type\"] == \"S\") and  element[\"url\"] != \"\"):\n",
    "            texts.append(options[element[\"type\"]](element[\"url\"]))\n",
    "        else:\n",
    "            print(\"Error during URL list analysis\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import stanza\n",
    "#from stanza.server import CoreNLPClient\n",
    "import os, nltk, re, random, time\n",
    "from nltk.parse import CoreNLPDependencyParser\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 23:10:57 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-03-03 23:10:57 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| lemma        | combined |\n",
      "| depparse     | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2022-03-03 23:10:57 INFO: Use device: cpu\n",
      "2022-03-03 23:10:57 INFO: Loading: tokenize\n",
      "2022-03-03 23:10:57 INFO: Loading: pos\n",
      "2022-03-03 23:10:57 INFO: Loading: lemma\n",
      "2022-03-03 23:10:57 INFO: Loading: depparse\n",
      "2022-03-03 23:10:58 INFO: Loading: constituency\n",
      "2022-03-03 23:10:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse,constituency')\n",
    "\n",
    "def vrbobj_pairs(text):\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sentences:\n",
    "        pairs = extrapolatePairs(sentence.words)\n",
    "        return pairs\n",
    "\n",
    "def extrapolatePairs(words):\n",
    "    pairs = []\n",
    "    nouns = getNouns(words)\n",
    "    for noun in nouns:\n",
    "        verb = goBackToVerb(noun, words)\n",
    "        if(verb != -1 and validate(verb.lemma, noun.lemma)):\n",
    "            pairs.append((verb.lemma, noun.lemma,getWeightFor(verb.lemma,noun.lemma)))\n",
    "    return pairs\n",
    "\n",
    "def goBackToVerb(word, words):\n",
    "    while word.deprel != \"root\":\n",
    "        word = words[word.head-1]\n",
    "        if(word.upos == \"VERB\"):\n",
    "            return word;\n",
    "    return -1\n",
    "\n",
    "def getNouns(words):\n",
    "    toReturn = []\n",
    "    for word in words:\n",
    "        if(word.upos == \"NOUN\"):\n",
    "            toReturn.append(word)\n",
    "    return toReturn\n",
    "\n",
    "def validate(verb,noun):\n",
    "    return 1\n",
    "\n",
    "def getWeightFor(verb,noun):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import os, nltk, re, random, time\n",
    "from nltk.parse import CoreNLPDependencyParser\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading goals and targets\n",
    "# goal regex: Goal ([0-9]+): ([a-zA-Z0-9-,.:! ]+) /// g1 = goal number /// g2 = goal text\n",
    "# target regex: [0-9]+.[0-9]+: ([a-zA-Z0-9-,.:! ]+) /// g1 = target text\n",
    "sdgir = dict() # SDG info raw list\n",
    "classifier = {} # dictionary of classifiers goal(key)->classifier(entry)\n",
    "tpairs = {} # the storage of verb-object pairs for targets \n",
    "tdict = {} # the storage of verb-object pairs for sentences in text\n",
    "\n",
    "def initialize():\n",
    "    preload()\n",
    "    init_classifiers()\n",
    "    #printGeneratedCouples()\n",
    "    print(\"\\n INITIALIZATION COMPLETED \\n\")\n",
    "\n",
    "def printGeneratedCouples():\n",
    "    for goal in sdgir.keys():\n",
    "        for target in sdgir[goal][1]:\n",
    "            print(\"\\n\")\n",
    "            vrbobj_pairs(\"We want to \" + target.lower())\n",
    "            print(\"\\n\")\n",
    "\n",
    "def preload():\n",
    "    for entry in os.listdir('./data/sdgs'):\n",
    "        file = open('./data/sdgs/' + entry)\n",
    "        line = file.readline()\n",
    "        gm = re.match(r'Goal ([0-9]+): ([^\\n]+)', line)\n",
    "        goal = int(gm.group(1))\n",
    "        sdgir[goal] = (gm.group(2), [])\n",
    "        file.readline()\n",
    "        while line:\n",
    "            tm = re.match(r'[0-9]+.[0-9]+: ([^\\n]+)', line)\n",
    "            if tm:\n",
    "                sdgir[goal][1].append(tm.group(1))\n",
    "            line = file.readline()\n",
    "        file.close()\n",
    "\n",
    "# creating feature extractor based on verb-object pair overlap\n",
    "def feature_extractor(goal, text):\n",
    "    features = {} # features\n",
    "    fc = 0\n",
    "    pairs = []\n",
    "    if text in tdict.keys():\n",
    "        pairs = tdict[text]\n",
    "    else:\n",
    "        tdict[text] = pairs = vrbobj_pairs(text)\n",
    "    for target in tpairs[goal]:\n",
    "        features['contains(%s)' % str(target)] = False\n",
    "        for p in pairs:\n",
    "            vflag, oflag = False, False\n",
    "            for ss in wn.synsets(target[0]):\n",
    "                if p[0] in ss.lemma_names():\n",
    "                    vflag = True\n",
    "                    break\n",
    "            if not vflag:\n",
    "                continue\n",
    "            for ss in wn.synsets(target[1]):\n",
    "                if p[1] in ss.lemma_names():\n",
    "                    oflag = True\n",
    "                    break\n",
    "            if vflag and oflag:\n",
    "                features['contains(%s)' % str(target)] = True\n",
    "                break\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining and training classifier\n",
    "def init_classifiers():\n",
    "    # initialization of the storage of verb-object pairs for targets\n",
    "    tpairs.clear()\n",
    "    for goal in sdgir.keys():\n",
    "        tpairs[goal] = []\n",
    "        print(\"generating tpairs for goal\",goal,\"...\")\n",
    "        for target in sdgir[goal][1]:\n",
    "            tpairs[goal] += vrbobj_pairs(\"We want to \" + target.lower())\n",
    "    # defining classifier\n",
    "    labeled_sent = [(\"We want to \" + target.lower(), goal) for goal in sdgir.keys() for target in sdgir[goal][1]]\n",
    "    random.shuffle(labeled_sent)\n",
    "    tdict.clear()\n",
    "    print(\"generating feature sets...\")\n",
    "    for goal in sdgir.keys():\n",
    "        featuresets = [(feature_extractor(goal, e), g == goal) for (e, g) in labeled_sent]\n",
    "        print('Feature sets generated for goal {}'.format(goal))\n",
    "        train_set = featuresets[:70]\n",
    "        classifier[goal] = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sdg(text):   \n",
    "    tdict.clear() \n",
    "    for goal in sdgir.keys():\n",
    "        ans = classifier[goal].classify(feature_extractor(goal, text))\n",
    "        if ans:\n",
    "            print(\"{}: {}\".format(goal, sdgir[goal][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating tpairs for goal 2 ...\n",
      "generating tpairs for goal 15 ...\n",
      "generating tpairs for goal 4 ...\n",
      "generating tpairs for goal 9 ...\n",
      "generating tpairs for goal 14 ...\n",
      "generating tpairs for goal 16 ...\n",
      "generating tpairs for goal 3 ...\n",
      "generating tpairs for goal 6 ...\n",
      "generating tpairs for goal 11 ...\n",
      "generating tpairs for goal 7 ...\n",
      "generating tpairs for goal 12 ...\n",
      "generating tpairs for goal 1 ...\n",
      "generating tpairs for goal 5 ...\n",
      "generating tpairs for goal 17 ...\n",
      "generating tpairs for goal 13 ...\n",
      "generating tpairs for goal 10 ...\n",
      "generating tpairs for goal 8 ...\n",
      "generating feature sets...\n",
      "Feature sets generated for goal 2\n",
      "Feature sets generated for goal 15\n",
      "Feature sets generated for goal 4\n",
      "Feature sets generated for goal 9\n",
      "Feature sets generated for goal 14\n",
      "Feature sets generated for goal 16\n",
      "Feature sets generated for goal 3\n",
      "Feature sets generated for goal 6\n",
      "Feature sets generated for goal 11\n",
      "Feature sets generated for goal 7\n",
      "Feature sets generated for goal 12\n",
      "Feature sets generated for goal 1\n",
      "Feature sets generated for goal 5\n",
      "Feature sets generated for goal 17\n",
      "Feature sets generated for goal 13\n",
      "Feature sets generated for goal 10\n",
      "Feature sets generated for goal 8\n",
      "\n",
      " INITIALIZATION COMPLETED \n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = retrieve()\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SINGLE TASK COMPLETED \n",
      " \n",
      "\n",
      " SINGLE TASK COMPLETED \n",
      " \n",
      "\n",
      " SINGLE TASK COMPLETED \n",
      " \n",
      "\n",
      " THE ANALYZES HAVE BEEN COMPLETED \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    check_sdg(text)\n",
    "    print(\"\\n SINGLE TASK COMPLETED \\n \")\n",
    "print(\"\\n THE ANALYZES HAVE BEEN COMPLETED \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
