{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text retriever\n",
    "part written by Stefano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "urls = []\n",
    "\n",
    "def fillURLs():\n",
    "    with open('urlsList.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            urls.append(line)\n",
    "\n",
    "\n",
    "def text_retriever():\n",
    "    fillURLs()\n",
    "    for url in urls:\n",
    "        response = requests.get(str(url))\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "        return soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "part written by Filip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import os, nltk, re, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading goals and targets\n",
    "# goal regex: Goal ([0-9]+): ([a-zA-Z0-9-,.:! ]+) /// g1 = goal number /// g2 = goal text\n",
    "# target regex: [0-9]+.[0-9]+: ([a-zA-Z0-9-,.:! ]+) /// g1 = target text\n",
    "sdgir = dict() # SDG info raw list\n",
    "for entry in os.listdir('./data/sdgs'):\n",
    "    file = open('./data/sdgs/' + entry)\n",
    "    line = file.readline()\n",
    "    gm = re.match(r'Goal ([0-9]+): ([^\\n]+)', line)\n",
    "    goal = int(gm.group(1))\n",
    "    sdgir[goal] = (gm.group(2), [])\n",
    "    file.readline()\n",
    "    while line:\n",
    "        tm = re.match(r'[0-9]+.[0-9]+: ([^\\n]+)', line)\n",
    "        if tm:\n",
    "            sdgir[goal][1].append(tm.group(1))\n",
    "        line = file.readline()\n",
    "    file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for features extractor\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  PP: {<IN><DT|JJ|NN.*>+}\n",
    "  NP: {<DT|JJ|NN.*|CD>+}\n",
    "  VP: {<RB>*<VB.*>(<TO|VB.*>|<NP|PP|,>+)*}\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "def vbnp_pairs(text):\n",
    "    sent = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    tree = cp.parse(sent)\n",
    "    ans = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'VP':\n",
    "            current_vb = None\n",
    "            last = \"\"\n",
    "            for st in subtree:\n",
    "                if type(st) is nltk.tree.Tree and st.label() == 'NP' and current_vb:\n",
    "                    np = \"\"\n",
    "                    for leave in st.leaves():\n",
    "                        np += leave[0] + \" \"\n",
    "                    ans.append((current_vb, np[:-1]))\n",
    "                    current_vb = None\n",
    "                elif type(st) is tuple and st[1].startswith('VB'):\n",
    "                    current_vb = st[0]\n",
    "                last = st[0]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating feature extractor based on vbnp pair overlap\n",
    "def feature_extractor(goal, text):\n",
    "    features = {} # features\n",
    "    fc = 0\n",
    "    pairs = vbnp_pairs(text)\n",
    "    for target in sdgir[goal][1]:\n",
    "        tpairs = vbnp_pairs(\"We want to \" + target.lower())\n",
    "        for tp in tpairs:\n",
    "            if tp in pairs:\n",
    "                fc += 1\n",
    "                break\n",
    "    features['vbnp_pair_overlap'] = fc\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature sets generated for goal 2\n",
      "Feature sets generated for goal 15\n",
      "Feature sets generated for goal 4\n",
      "Feature sets generated for goal 9\n",
      "Feature sets generated for goal 14\n",
      "Feature sets generated for goal 16\n",
      "Feature sets generated for goal 3\n",
      "Feature sets generated for goal 6\n",
      "Feature sets generated for goal 11\n",
      "Feature sets generated for goal 7\n",
      "Feature sets generated for goal 12\n",
      "Feature sets generated for goal 1\n",
      "Feature sets generated for goal 5\n",
      "Feature sets generated for goal 17\n",
      "Feature sets generated for goal 13\n",
      "Feature sets generated for goal 10\n",
      "Feature sets generated for goal 8\n"
     ]
    }
   ],
   "source": [
    "# creating classifier\n",
    "classifier = {}\n",
    "labeled_sent = [(\"We want to \" + target.lower(), goal) for goal in sdgir.keys() for target in sdgir[goal][1]]\n",
    "random.shuffle(labeled_sent)\n",
    "for goal in sdgir.keys():\n",
    "    featuresets = [(feature_extractor(goal, e), g == goal) for (e, g) in labeled_sent]\n",
    "    print('Feature sets generated for goal {}'.format(goal))\n",
    "    train_set = featuresets[:100]\n",
    "    classifier[goal] = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sdg(text):\n",
    "    for goal in sdgir.keys():\n",
    "        ans = classifier[goal].classify(feature_extractor(goal, text))\n",
    "        if ans:\n",
    "            print(\"{}: {}\".format(goal, sdgir[goal][0]))\n",
    "\n",
    "# check_sdg('We want to eradicate extreme poverty!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17: Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\n",
      "13: Take urgent action to combat climate change and its impacts\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    text = text_retriever()\n",
    "    check_sdg(text)\n",
    "    \n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
