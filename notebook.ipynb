{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def fillURLs():\n",
    "    with open('urlsList.json') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file(url):\n",
    "    #print(\"is a file\", url)\n",
    "    response = requests.get(str(url))\n",
    "    text = response.text\n",
    "    #print(text)\n",
    "    return text\n",
    "    \n",
    "def site(url):\n",
    "    #print(\"is a site\", url)\n",
    "    response = requests.get(str(url))\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "options = {\n",
    "    \"F\": file,\n",
    "    \"S\": site\n",
    "}\n",
    "\n",
    "def retrieve():\n",
    "    elements = fillURLs()\n",
    "    texts = []\n",
    "    for element in elements:\n",
    "        if( (element[\"type\"] == \"F\" or element[\"type\"] == \"S\") and  element[\"url\"] != \"\"):\n",
    "            texts.append(options[element[\"type\"]](element[\"url\"]))\n",
    "        else:\n",
    "            print(\"Error during URL list analysis\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import stanza\n",
    "#from stanza.server import CoreNLPClient\n",
    "import os, nltk, re, random, time\n",
    "from nltk.parse import CoreNLPDependencyParser\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 20:14:36 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-03-06 20:14:36 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| lemma        | combined |\n",
      "| depparse     | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2022-03-06 20:14:36 INFO: Use device: cpu\n",
      "2022-03-06 20:14:36 INFO: Loading: tokenize\n",
      "2022-03-06 20:14:36 INFO: Loading: pos\n",
      "2022-03-06 20:14:37 INFO: Loading: lemma\n",
      "2022-03-06 20:14:37 INFO: Loading: depparse\n",
      "2022-03-06 20:14:37 INFO: Loading: constituency\n",
      "2022-03-06 20:14:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse,constituency')\n",
    "\n",
    "with open('./data/options/blacklist.json') as f:\n",
    "    blObj = json.load(f)\n",
    "    blacklistedVerbs = blObj[\"blacklistedVerbs\"]\n",
    "    blacklistedNouns = blObj[\"blacklistedNouns\"]\n",
    "    blacklistedCouples = blObj[\"blacklistedCouples\"]\n",
    "\n",
    "def orderTuples(allPairs):\n",
    "    tmp = []\n",
    "    for pairs in allPairs:\n",
    "        tmp = tmp + pairs\n",
    "    tmp.sort(key=lambda x:x[1])\n",
    "    tmp.sort(key=lambda x:x[0])\n",
    "    return tmp\n",
    "\n",
    "def writePairsForSDG(sdg, pairs):\n",
    "    stringnum = \"\"\n",
    "    if(sdg < 10):\n",
    "        stringnum = \"0\"\n",
    "    stringnum = stringnum + str(sdg) \n",
    "    with open('./data/dataset/'+stringnum+'pairs.txt','w') as f:\n",
    "        for tup in pairs:\n",
    "            text = str(str(tup[0])+\" \"+str(tup[1])+\" \"+str(tup[2])+\"\\n\")\n",
    "            f.write(text)\n",
    "        \n",
    "\n",
    "def generateDatasetFor(sdgNum, texts):\n",
    "    allPairs = []\n",
    "    for text in texts:\n",
    "        pairs = vrbobj_pairs(text)\n",
    "        allPairs.append(pairs)\n",
    "    return allPairs\n",
    "\n",
    "\n",
    "def vrbobj_pairs(text):\n",
    "    doc = nlp(text)\n",
    "    allPairs = []\n",
    "    for sentence in doc.sentences:\n",
    "        pairs = extrapolatePairs(sentence.words)\n",
    "        #print(sentence.text)\n",
    "        allPairs = allPairs + pairs\n",
    "    return allPairs\n",
    "\n",
    "def extrapolatePairs(words):\n",
    "    pairs = []\n",
    "    nouns = getNouns(words)\n",
    "    for noun in nouns:\n",
    "        verb = goBackToVerb(noun, words)\n",
    "        if(verb != -1 and validate(verb.lemma, noun.lemma)):\n",
    "            pairs.append((verb.lemma, noun.lemma,getWeightFor(verb.lemma,noun.lemma)))\n",
    "    return pairs\n",
    "\n",
    "def goBackToVerb(word, words):\n",
    "    while word.deprel != \"root\":\n",
    "        word = words[word.head-1]\n",
    "        #This is an extra filter, verify if necessary\n",
    "        if(word.upos == \"NOUN\"):\n",
    "            return -1\n",
    "        if(word.upos == \"VERB\"):\n",
    "            return word;\n",
    "    return -1\n",
    "\n",
    "def getNouns(words):\n",
    "    toReturn = []\n",
    "    for word in words:\n",
    "        if(word.upos == \"NOUN\"):\n",
    "            toReturn.append(word)\n",
    "    return toReturn\n",
    "\n",
    "def validate(verb,noun):\n",
    "    if(verb in blacklistedVerbs):\n",
    "        return 0\n",
    "    if(noun in blacklistedNouns):\n",
    "        return 0\n",
    "    for couple in blacklistedCouples:\n",
    "        if(couple[\"verb\"] == verb and couple[\"noun\"] == noun):\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "def getWeightFor(verb,noun):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import os, nltk, re, random, time\n",
    "from nltk.parse import CoreNLPDependencyParser\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading goals and targets\n",
    "# goal regex: Goal ([0-9]+): ([a-zA-Z0-9-,.:! ]+) /// g1 = goal number /// g2 = goal text\n",
    "# target regex: [0-9]+.[0-9]+: ([a-zA-Z0-9-,.:! ]+) /// g1 = target text\n",
    "sdgir = dict() # SDG info raw list\n",
    "classifier = {} # dictionary of classifiers goal(key)->classifier(entry)\n",
    "tpairs = dict() # the storage of verb-object pairs for targets \n",
    "tdict = {} # the storage of verb-object pairs for sentences in text\n",
    "\n",
    "def initialize():\n",
    "    preload()\n",
    "    init_classifiers()\n",
    "    #printGeneratedCouples()\n",
    "    print(\"\\n INITIALIZATION COMPLETED \\n\")\n",
    "            \n",
    "def preload():\n",
    "    for entry in os.listdir('./data/sdgs'):\n",
    "        file = open('./data/sdgs/' + entry)\n",
    "        line = file.readline()\n",
    "        gm = re.match(r'Goal ([0-9]+): ([^\\n]+)', line)\n",
    "        goal = int(gm.group(1))\n",
    "        sdgir[goal] = gm.group(2)\n",
    "    for entry in os.listdir('./data/dataset'):\n",
    "        file = open('./data/dataset/' + entry)\n",
    "        goal = int(entry[0:2])\n",
    "        line = file.readline()\n",
    "        tpairs[goal] = []\n",
    "        while line:\n",
    "            tpairs[goal].append((line.split()[0], line.split()[1]))\n",
    "            line = file.readline()\n",
    "        file.close()\n",
    "\n",
    "# creating feature extractor based on verb-object pair overlap\n",
    "def feature_extractor(goal, text):\n",
    "    features = {} # features\n",
    "    fc = 0\n",
    "    pairs = []\n",
    "    if text in tdict.keys():\n",
    "        pairs = tdict[text]\n",
    "    else:\n",
    "        tdict[text] = pairs = vrbobj_pairs(text)\n",
    "    for target in tpairs[goal]:\n",
    "        features['contains(%s)' % str(target)] = False\n",
    "        for p in pairs:\n",
    "            vflag, oflag = False, False\n",
    "            for ss in wn.synsets(target[0]):\n",
    "                if p[0] in ss.lemma_names():\n",
    "                    vflag = True\n",
    "                    break\n",
    "            if not vflag:\n",
    "                continue\n",
    "            for ss in wn.synsets(target[1]):\n",
    "                if p[1] in ss.lemma_names():\n",
    "                    oflag = True\n",
    "                    break\n",
    "            if vflag and oflag:\n",
    "                features['contains(%s)' % str(target)] = True\n",
    "                break\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining and training classifier\n",
    "def init_classifiers():\n",
    "    # defining classifier\n",
    "    labeled_sent = [(\"We want to \" + target.lower(), goal) for goal in sdgir.keys() for target in sdgir[goal][1]]\n",
    "    random.shuffle(labeled_sent)\n",
    "    tdict.clear()\n",
    "    print(\"generating feature sets...\")\n",
    "    for goal in sdgir.keys():\n",
    "        featuresets = [(feature_extractor(goal, e), g == goal) for (e, g) in labeled_sent]\n",
    "        print('Feature sets generated for goal {}'.format(goal))\n",
    "        train_set = featuresets\n",
    "        classifier[goal] = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sdg(text):   \n",
    "    tdict.clear() \n",
    "    for goal in sdgir.keys():\n",
    "        ans = classifier[goal].classify(feature_extractor(goal, text))\n",
    "        if ans:\n",
    "            print(\"{}: {}\".format(goal, sdgir[goal]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating feature sets...\n",
      "Feature sets generated for goal 2\n",
      "Feature sets generated for goal 15\n",
      "Feature sets generated for goal 4\n",
      "Feature sets generated for goal 9\n",
      "Feature sets generated for goal 14\n",
      "Feature sets generated for goal 16\n",
      "Feature sets generated for goal 3\n",
      "Feature sets generated for goal 6\n",
      "Feature sets generated for goal 11\n",
      "Feature sets generated for goal 7\n",
      "Feature sets generated for goal 12\n",
      "Feature sets generated for goal 1\n",
      "Feature sets generated for goal 5\n",
      "Feature sets generated for goal 17\n",
      "Feature sets generated for goal 13\n",
      "Feature sets generated for goal 10\n",
      "Feature sets generated for goal 8\n",
      "\n",
      " INITIALIZATION COMPLETED \n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = retrieve()\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: End hunger, achieve food security and improved nutrition and promote sustainable agriculture\n",
      "15: Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\n",
      "4: Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\n",
      "9: Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\n",
      "14: Conserve and sustainably use the oceans, seas and marine resources for sustainable development 14.1\n",
      "16: Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\n",
      "3: Ensure healthy lives and promote well-being for all at all ages\n",
      "6: Ensure availability and sustainable management of water and sanitation for all\n",
      "7: Ensure access to affordable, reliable, sustainable and modern energy for all\n",
      "12: Ensure sustainable consumption and production patterns\n",
      "1: End poverty in all its forms everywhere\n",
      "17: Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\n",
      "13: Take urgent action to combat climate change and its impacts\n",
      "10: Reduce inequality within and among countries\n",
      "8: Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\n",
      "\n",
      " SINGLE TASK COMPLETED \n",
      " \n",
      "2: End hunger, achieve food security and improved nutrition and promote sustainable agriculture\n",
      "15: Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\n",
      "4: Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\n",
      "9: Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\n",
      "14: Conserve and sustainably use the oceans, seas and marine resources for sustainable development 14.1\n",
      "16: Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\n",
      "3: Ensure healthy lives and promote well-being for all at all ages\n",
      "6: Ensure availability and sustainable management of water and sanitation for all\n",
      "11: Make cities and human settlements inclusive, safe, resilient and sustainable\n",
      "7: Ensure access to affordable, reliable, sustainable and modern energy for all\n",
      "12: Ensure sustainable consumption and production patterns\n",
      "1: End poverty in all its forms everywhere\n",
      "5: Achieve gender equality and empower all women and girls\n",
      "17: Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\n",
      "13: Take urgent action to combat climate change and its impacts\n",
      "10: Reduce inequality within and among countries\n",
      "8: Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\n",
      "\n",
      " SINGLE TASK COMPLETED \n",
      " \n",
      "\n",
      " SINGLE TASK COMPLETED \n",
      " \n",
      "\n",
      " THE ANALYZES HAVE BEEN COMPLETED \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    check_sdg(text)\n",
    "    print(\"\\n SINGLE TASK COMPLETED \\n \")\n",
    "print(\"\\n THE ANALYZES HAVE BEEN COMPLETED \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
