{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text retriever\n",
    "part written by Stefano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def fillURLs():\n",
    "    with open('urlsList.json') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def file(url):\n",
    "    #print(\"is a file\", url)\n",
    "    response = requests.get(str(url))\n",
    "    text = response.text\n",
    "    return text\n",
    "    #print(text)\n",
    "    \n",
    "def site(url):\n",
    "    #print(\"is a site\", url)\n",
    "    response = requests.get(str(url))\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "    #print(text)\n",
    "\n",
    "options = {\n",
    "    \"F\": file,\n",
    "    \"S\": site\n",
    "}\n",
    "\n",
    "def text_retriever():\n",
    "    elements = fillURLs()\n",
    "    for element in elements:\n",
    "        if( (element[\"type\"] == \"F\" or element[\"type\"] == \"S\") and  element[\"url\"] != \"\"):\n",
    "            return options[element[\"type\"]](element[\"url\"])\n",
    "        else:\n",
    "            print(\"Error during URL list analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "part written by Filip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import os, nltk, re, random, time\n",
    "from nltk.parse import CoreNLPDependencyParser\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading goals and targets\n",
    "# goal regex: Goal ([0-9]+): ([a-zA-Z0-9-,.:! ]+) /// g1 = goal number /// g2 = goal text\n",
    "# target regex: [0-9]+.[0-9]+: ([a-zA-Z0-9-,.:! ]+) /// g1 = target text\n",
    "sdgir = dict() # SDG info raw list\n",
    "for entry in os.listdir('./data/sdgs'):\n",
    "    file = open('./data/sdgs/' + entry)\n",
    "    line = file.readline()\n",
    "    gm = re.match(r'Goal ([0-9]+): ([^\\n]+)', line)\n",
    "    goal = int(gm.group(1))\n",
    "    sdgir[goal] = (gm.group(2), [])\n",
    "    file.readline()\n",
    "    while line:\n",
    "        tm = re.match(r'[0-9]+.[0-9]+: ([^\\n]+)', line)\n",
    "        if tm:\n",
    "            sdgir[goal][1].append(tm.group(1))\n",
    "        line = file.readline()\n",
    "    file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1\n",
    "The version using just standard parser  \n",
    "*deprecated*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for features extractor\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  PP: {<IN><DT|JJ|NN.*>+}\n",
    "  NP: {<DT|JJ|NN.*|CD>+}\n",
    "  VP: {<RB>*<VB.*>(<TO|VB.*>|<NP|PP|,>+)*}\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "def vrbobj_pairs(text):\n",
    "    sent = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    tree = cp.parse(sent)\n",
    "    ans = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'VP':\n",
    "            current_vb = None\n",
    "            last = \"\"\n",
    "            for st in subtree:\n",
    "                if type(st) is nltk.tree.Tree and st.label() == 'NP' and current_vb:\n",
    "                    np = \"\"\n",
    "                    for leave in st.leaves():\n",
    "                        np += leave[0] + \" \"\n",
    "                    ans.append((current_vb, np[:-1]))\n",
    "                    current_vb = None\n",
    "                elif type(st) is tuple and st[1].startswith('VB'):\n",
    "                    current_vb = st[0]\n",
    "                last = st[0]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2\n",
    "The version using Stanford CoreNLP parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vrbobj_pairs(text):\n",
    "    dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "    sent = nltk.word_tokenize(text)\n",
    "    parse, = dep_parser.parse(sent)\n",
    "    ans = []\n",
    "    for governor, dep, dependent in parse.triples():\n",
    "        if dep == 'obj':\n",
    "            ans.append((governor[0], dependent[0]))\n",
    "    return ans\n",
    "\n",
    "def selfreference(text):\n",
    "    parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating feature extractor based on verb-object pair overlap\n",
    "tpairs = {} # the storage of verb-object pairs for targets \n",
    "for goal in sdgir.keys():\n",
    "    tpairs[goal] = []\n",
    "    for target in sdgir[goal][1]:\n",
    "        tpairs[goal] += vrbobj_pairs(\"We want to \" + target.lower())\n",
    "        \n",
    "tdict = {} # the storage of verb-object pairs for sentences in text\n",
    "        \n",
    "def feature_extractor(goal, text):\n",
    "    features = {} # features\n",
    "    fc = 0\n",
    "    pairs = []\n",
    "    if text in tdict.keys():\n",
    "        pairs = tdict[text]\n",
    "    else:\n",
    "        tdict[text] = pairs = vrbobj_pairs(text)\n",
    "    for target in tpairs[goal]:\n",
    "        for p in pairs:\n",
    "            vflag, oflag = False, False\n",
    "            for ss in wn.synsets(target[0]):\n",
    "                if p[0] in ss.lemma_names():\n",
    "                    vflag = True\n",
    "                    break\n",
    "            if not vflag:\n",
    "                continue\n",
    "            for ss in wn.synsets(target[1]):\n",
    "                if p[1] in ss.lemma_names():\n",
    "                    oflag = True\n",
    "                    break\n",
    "            if vflag and oflag:\n",
    "                fc += 1\n",
    "    features['vrbobj_pair_overlap'] = fc\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature sets generated for goal 2\n",
      "Feature sets generated for goal 15\n",
      "Feature sets generated for goal 4\n",
      "Feature sets generated for goal 9\n",
      "Feature sets generated for goal 14\n",
      "Feature sets generated for goal 16\n",
      "Feature sets generated for goal 3\n",
      "Feature sets generated for goal 6\n",
      "Feature sets generated for goal 11\n",
      "Feature sets generated for goal 7\n",
      "Feature sets generated for goal 12\n",
      "Feature sets generated for goal 1\n",
      "Feature sets generated for goal 5\n",
      "Feature sets generated for goal 17\n",
      "Feature sets generated for goal 13\n",
      "Feature sets generated for goal 10\n",
      "Feature sets generated for goal 8\n"
     ]
    }
   ],
   "source": [
    "# creating and training classifier\n",
    "classifier = {}\n",
    "labeled_sent = [(\"We want to \" + target.lower(), goal) for goal in sdgir.keys() for target in sdgir[goal][1]]\n",
    "random.shuffle(labeled_sent)\n",
    "tdict.clear()\n",
    "for goal in sdgir.keys():\n",
    "    featuresets = [(feature_extractor(goal, e), g == goal) for (e, g) in labeled_sent]\n",
    "    print('Feature sets generated for goal {}'.format(goal))\n",
    "    train_set = featuresets[:70]\n",
    "    classifier[goal] = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sdg(text):\n",
    "    tdict.clear()\n",
    "    for goal in sdgir.keys():\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            ans = classifier[goal].classify(feature_extractor(goal, sent))\n",
    "            if ans:\n",
    "                print(\"{}: {}\".format(goal, sdgir[goal][0]))\n",
    "\n",
    "# check_sdg('We want to improve education!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text retrieved in 1.1769111429998702 seconds\n",
      "16: Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\n",
      "13: Take urgent action to combat climate change and its impacts\n",
      "Text checked in 10.456094774999883 seconds\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    tic = time.perf_counter()\n",
    "    text = text_retriever()\n",
    "    toc = time.perf_counter()\n",
    "    print(\"Text retrieved in {} seconds\".format(toc - tic))\n",
    "    tic = time.perf_counter()\n",
    "    check_sdg(text)\n",
    "    toc = time.perf_counter()\n",
    "    print(\"Text checked in {} seconds\".format(toc - tic))\n",
    "    \n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
